# ğŸ¤ Assistive Sign Language Converter
### *Bridging Communication Gaps for Hearing and Speech Impaired Individuals*

<div align="center">

![System Architecture](docs/images/system_architecture.png)

**ğŸ¯ Real-time Sign Language Translation | ğŸ¥ Healthcare-Focused | ğŸ¤– AI-Powered**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-Deep%20Learning-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

</div>

---

## ğŸŒŸ Project Highlights

<table>
<tr>
<td width="50%">

### ğŸ’¡ Innovation
- **Real-time Translation**: Convert sign language to text/speech instantly
- **Bi-directional Communication**: Two-way doctor-patient interaction
- **Deep Learning Powered**: State-of-the-art Transformer architecture
- **0.0478 BLUE SCORE**: With Test Data Performance

</td>
<td width="50%">

### ğŸ¯ Impact
- Enhanced healthcare accessibility
- Improved patient-provider communication
- Reduced communication barriers
- Scalable and deployable solution

</td>
</tr>
</table>

---

## ğŸ‘¥ Team

<div align="center">

### ğŸ“ Authors
**GUNARATHNA L.P.N.** (2020/E/046) â€¢ **SOMARATHNA S.V.A.P.K.** (2020/E/212)

### ğŸ‘¨â€ğŸ« Supervisors
**Dr. T. Mukunthan** â€¢ **Prof. M. K. Ahilan** â€¢ **Mr. R. Valluvan**  
*Department of Electrical and Electronic Engineering, University of Jaffna*

</div>

---

## ğŸ¬ System in Action

<div align="center">

### Doctor Interface
![Doctor Interface](docs/images/doctor_interface.png)

### Patient Interface
![Patient Interface](docs/images/patient_interface.png)

</div>

---

## ğŸ—ï¸ System Architecture

<div align="center">

![Complete System Architecture](docs/images/system_architecture2.png)

**Client-Server Architecture with Integrated Deep Learning Models**

</div>

### ğŸ”„ Communication Flow

```mermaid
graph LR
    A[ğŸ‘¨â€ğŸ¦¯ Patient] -->|Signs| B[ğŸ“¹ Video Capture]
    B -->|Stream| C[ğŸ”Œ Client]
    C -->|Network| D[ğŸ¥ Server]
    D -->|Process| E[ğŸ§  AI Model]
    E -->|Translate| F[ğŸ“ Text/Speech]
    F -->|Display| G[ğŸ‘¨â€âš•ï¸ Doctor]
    G -->|Response| H[ğŸ”Š Voice]
    H -->|Convert| D
    D -->|Text| C
    C -->|Display| A
```

---

## ğŸ“Š Performance Metrics

<div align="center">

### ğŸ† Model Comparison

| Model Architecture | Train Score | Validation | Test Score | Status |
|:------------------:|:-----------:|:----------:|:----------:|:------:|
| ğŸ”µ **T5 Decoder** | 0.0137 | 0.0133 | 0.0129 | âœ… Good |
| ğŸŸ¡ **LSTM** | 0.0388 | 0.0299 | 0.0312 | âœ… Better |
| ğŸŸ¢ **Transformer** | **0.0689** | **0.0459** | **0.0478** | ğŸ† **Best** |

### Training Progress
![Training Loss Curve](docs/images/training_loss.png)

### Feature Extraction Visualization
![Feature Visualization](docs/images/feature_viz.png)

</div>

---


## ğŸš€ Quick Start

### ğŸ“‹ Prerequisites

```bash
# Core Dependencies
Python 3.8+
PyTorch
OpenCV
MediaPipe
```

### âš¡ Installation

```bash
# 1. Clone the repository
git clone https://github.com/PramodGunarathna/assistive-sign-language-converter.git
cd assistive-sign-language-converter

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# OR
.\venv\Scripts\activate  # Windows

# 3. Install dependencies
pip install -r requirements.txt
pip install -r client/requirements.txt
pip install -r doctor/requirements.txt
pip install -r model_integration/requirements.txt
```

### ğŸƒâ€â™‚ï¸ Running the System

<table>
<tr>
<td width="33%">

#### 1ï¸âƒ£ Start Doctor Interface
```bash
python run_doctor.py
```

</td>
<td width="33%">

#### 2ï¸âƒ£ Launch Patient Client
```bash
python run_patient.py
```

</td>
<td width="33%">

#### 3ï¸âƒ£ Test Connection
```bash
python test_connection.py
```

</td>
</tr>
</table>

---

## ğŸ§  Technical Architecture

### ğŸ”‘ Key Components

<table>
<tr>
<td width="33%">

#### ğŸ“¹ Feature Extraction
- **I3D ConvNet**: 3D spatial-temporal analysis
- **MediaPipe**: Skeleton extraction
- **Optical Flow**: Motion tracking

</td>
<td width="33%">

#### ğŸ¤– AI Models
- **Transformer**: Context understanding
- **LSTM**: Sequential learning
- **Encoder-Decoder**: Translation pipeline

</td>
<td width="33%">

#### ğŸŒ Communication
- **Socket-based**: Real-time streaming
- **Bi-directional**: Two-way translation
- **Low-latency**: <1s response

</td>
</tr>
</table>

### ğŸ“ Repository Structure

```
ğŸ“¦ sign-language-converter/
â”œâ”€â”€ ğŸ§  best_model2_with LSTM.pth              # LSTM model weights
â”œâ”€â”€ ğŸ¤– best_model2_with_Transformers_28000.pth # Transformer weights
â”œâ”€â”€ ğŸ‘¨â€âš•ï¸ client/                                # Patient-side application
â”‚   â”œâ”€â”€ patient_client.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ ğŸ¥ doctor/                                # Doctor-side application
â”‚   â”œâ”€â”€ server.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ ğŸ“Š features/                              # Extracted video features
â”œâ”€â”€ ğŸ”Œ model_integration/                     # AI model integration
â”œâ”€â”€ ğŸ¥ pytorch-i3d/                           # I3D implementation
â”œâ”€â”€ ğŸ“ saved_tokenizer_T5Decoder/             # Text tokenizer
â””â”€â”€ ğŸ“š docs/
    â”œâ”€â”€ images/                               # System screenshots
    â””â”€â”€ publications/                         # Research papers
```

---

## ğŸ”¬ Research Methodology

### 1ï¸âƒ£ **Data Collection**
- Used ASL How to Sign dataset for initial model training
- Analysis of sign language patterns from the dataset
- Trained models on ASL dataset for telehealth applications
- **Future Work:** Collect Sri Lankan Sign Language (SSL) dataset and create localized model

### 2ï¸âƒ£ **Model Development**
- I3D feature extraction from videos
- Transformer-based architecture design
- LSTM temporal modeling integration

### 3ï¸âƒ£ **System Integration**
- Real-time video processing pipeline
- Socket-based communication protocol
- Voice-to-text feedback system

### 4ï¸âƒ£ **Validation**
- Clinical trials in healthcare settings
- Performance metric evaluation
- User satisfaction assessment

---

## ğŸ¯ Key Features

<div align="center">

| Feature | Description | Status |
|:--------|:------------|:------:|
| ğŸ¥ **Real-time Processing** | Live video capture and translation | âœ… |
| ğŸ”„ **Bi-directional** | Doctor responses translated back | âœ… |
| ğŸ§  **Deep Learning** | State-of-the-art AI models | âœ… |
| ğŸ¥ **Healthcare Focus** | Medical terminology support | âœ… |
| ğŸ“± **User-friendly** | Intuitive interfaces | âœ… |
| ğŸ”’ **Secure** | Encrypted communication | âœ… |
| âš¡ **Low Latency** | <150ms response time | âœ… |
| ğŸŒ **Scalable** | Cloud-ready architecture | ğŸ”„ |

</div>

---

## ğŸ”® Future Roadmap

<table>
<tr>
<td width="50%">

### ğŸ¯ Short-term Goals
- ğŸ”„ Sri Lankan Sign Language (SSL) dataset collection and localized model development
- âœ… Mobile app development
- âœ… Cloud deployment
- âœ… Enhanced avatar system

</td>
<td width="50%">

### ğŸš€ Long-term Vision
- ğŸŒ Multi-language support
- ğŸ¤– Advanced AI models
- ğŸ‘¤ Avatar-based model to convert doctor voice into sign language
- ğŸ“Š Analytics dashboard
- ğŸ¢ Enterprise solutions

</td>
</tr>
</table>

---

## ğŸ“š Documentation

### ğŸ“– Available Resources

<div align="center">

| Document | Description | Link |
|:---------|:------------|:----:|
| ğŸ“˜ **Thesis** | Complete research documentation | <a href="docs/publications/Thesis_2020E046_2020E212.pdf" download>PDF</a> |
| ğŸ“„ **Research Paper** | Published findings | <a href="docs/publications/research_paper.pdf" download>PDF</a> |
| ğŸ¨ **Project Poster** | Visual summary | <a href="docs/publications/project_poster.pdf" download>PDF</a> |

</div>

---

## ğŸ·ï¸ Technology Stack

<div align="center">

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-27338e?style=for-the-badge&logo=OpenCV&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)

**Core Technologies**: Deep Learning â€¢ Computer Vision â€¢ NLP â€¢ Socket Programming â€¢ Real-time Processing

**AI Models**: Transformer â€¢ LSTM â€¢ GRU â€¢ I3D ConvNet â€¢ T5 Decoder

**Frameworks**: MediaPipe â€¢ HOW2SIGN Dataset â€¢ Sequence-to-Sequence Learning

</div>

---

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. ğŸ’¾ Commit changes (`git commit -m 'Add AmazingFeature'`)
4. ğŸ“¤ Push to branch (`git push origin feature/AmazingFeature`)
5. ğŸ‰ Open a Pull Request

---

## ğŸ“§ Contact

<div align="center">

**Questions or suggestions?**

ğŸ“§ Email: [pramodnadishka.l@gmail.com](mailto:pramodnadishka.l@gmail.com)  
ğŸ”— Project Link: [https://github.com/PramodGunarathna/assistive-sign-language-converter](https://github.com/PramodGunarathna/assistive-sign-language-converter)

</div>

---

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

### ğŸŒŸ If you find this project helpful, please give it a star! ğŸŒŸ

**Made with â¤ï¸ by the University of Jaffna Team**

![Visitors](https://visitor-badge.laobi.icu/badge?page_id=yourusername.sign-language-converter)

</div>